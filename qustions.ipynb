{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Questions for interview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Machine learning\n",
    "### Machine learning is the study about algorithms that are used to perform a specific task without using explicit instructions. The information about how to perform a task is learned from data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Unsupervised and Supervised learning\n",
    "* ### Unsupervised learning algorithms experience a dataset containing many features, then learn useful properties of the structure of this dataset. In the context of deep learning, we usually want to learn the entire probability distribution that generated a dataset, whether explicitly, as a density estimation, or implicitly, for tasks like synthesis (GAN) or denoising (AE). Some other unsupervised learning algorithms perform other roles, like clustering, which consists of dividing the dataset into clusters of similar examples.\n",
    "* ### Supervised learning algorithms experience a dataset containing features, but each example is also associated with a label or target. A supervised learning algorithm can study the dataset and learn how to predict labels for a new examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Difference between optimization and machine learning\n",
    "\n",
    "### Typically, during training a machine learning model, we have access to a training set and we can compute some error measure on the training set, called the training error; and we reduce this training error. So far, it is simply an optimization problem. What separates machine learning from optimization is that we want the generalization error, also called the test error, to be low as well. The generalization error is deﬁned as the expected value of the error on a previously before unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explain overfitting and describe what steps can be taken to reduce overfitting.\n",
    "\n",
    "### Usually during training a model, we have access to a training set and we can compute some error measure on the training set, called the training error; and we reduce this training error. The goal is that the model will also perform well on predicting the output when fed test data that was not encountered during its training. So there is two situation: underﬁtting and overﬁtting.\n",
    "* ### Underﬁtting occurs when the model is not able to obtain a suﬃciently low error value on the training set. Underfitting would occur, for example, when fitting a linear model to non-linear data. \n",
    "* ### Overﬁtting occurs when the gap between the training error and test error is too large. So our model doesn’t generalize well from our training data to unseen data. It can be seen as fitting irrelevant information or random noise from training dataset.\n",
    "\n",
    "## Several techniques are available to reduce overﬁtting: \n",
    "1. ### Cross-validation is way to determine overfitting. You use your training data to generate multiple mini train-test splits and use train part to train model and than evalute its performans on test set.\n",
    "2. ### We can altering model capacity, try to make it simplier. For example add some resriction on model's parameters this techniques called regularization. You could prune a decision tree or reduce amount of parameter of neural network.\n",
    "3. ### Cleaning data from outliers. And we can also remove irrelevant input features.\n",
    "4. ### Early stopping refers stopping the training process when perfomens on test error decreasing.\n",
    "5. ### Ensembles are machine learning methods for combining predictions from multiple separate models. Model should be independent and better than guessing.\n",
    "6. ### Dataset Augmentation is a process of transformations of our data that do not change the class. Thus we increase size of training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Bias and Variance trade off\n",
    "\n",
    "### Bias and variance measure two diﬀerent sources of error in an estimator. Bias measures the expected deviation from the true value of the function or parameter. Variance on the other hand, provides a measure of the deviation from the expected estimator value that any particular sampling of the data is likely to cause. The relationship between bias and variance is tightly linked to the machine learning concepts of capacity, underﬁtting and overﬁtting. When generalization error is measured by the MSE (where bias and variance are meaningful componentsof generalization error), increasing capacity tends to increase variance and decrease bias.\n",
    "* ### The bias is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\n",
    "* ### The variance is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Out-of-bag\n",
    "###  Out-of-bag estimate, is a method of measuring the prediction error of random forests, boosted decision trees, and other machine learning models utilizing bootstrap aggregating (bagging) to sub-sample data samples used for training. Subsampling allows one to define an out-of-bag estimate of the prediction performance improvement by evaluating predictions on those observations which were not used in the building of the next base learner. Out-of-bag estimates help avoid the need for an independent validation dataset, but often underestimates actual performance improvement and the optimal number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## What is the \"Curse of Dimensionality?\"\n",
    "### The number of possible distinct conﬁgurations of a set of variables increases exponentially as the number of variables increases. So the number of conﬁgurations is huge, much larger than our number of examples, a typical grid cell has no training example associated with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## How do you split your data between training and validation?\n",
    "### Dividing the dataset into a ﬁxed training set and a ﬁxed test set can be problematic if it results in the test set being small. A small test set implies statistical uncertaint around the estimated average test error, making it diﬃcult to claim that algorithm A works better than algorithm B on the given task. When the dataset has hundreds of thousands of examples or more, this is not a serious issue. \n",
    "### When the dataset is too small, are alternative procedures enable one to use all the examples in the estimation of the mean test error, at the price of increased computational cost. These procedures are based on the idea of repeating the training and testing computation on diﬀerent randomly chosen subsets or splits of the original dataset. The most common of these is the k-fold cross-validation procedure in which a partition of the dataset is formed by splitting it into k nonoverlapping subsets. The test error may then be estimatedby taking the average test error across k trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Time series\n",
    "### A time series is a series of data points indexed in time order.\n",
    "\n",
    "### Autocorrelation is the correlation of a signal with a delayed copy of itself as a function of delay. \n",
    "\n",
    "### For stationary time series its statistics do not depend in which time period we calculate them. To make our data stationary we can use Box-Cox transformation, differencing maybe (season differencing). Dickey–Fuller test is used to check stationary.\n",
    "\n",
    "### Autoregression is a model that depends linearly on its own previous values and constant.\n",
    "\n",
    "### The moving-average model specifies that the output variable depends linearly on the current and various past values of a stochastic term.\n",
    "\n",
    "### ARMA(p, q) is combination of two models above, where p is an amount of lags used AR model and q for MA model. ARIMA(p, d, q) is ARMA(p, q), where d is the degree of differencing (the number of times the data have had past values subtracted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Recommendation system\n",
    "### RS  is a system that seeks to predict the \"rating\" or \"preference\" a user would give to an item. RS typically produce a list of recommendations in one of two ways – through collaborative filtering or through content-based filtering. \n",
    "\n",
    "### Collaborative filtering approaches build a model from a user's past behaviour (items previously purchased or selected and/or numerical ratings given to those items) as well as similar decisions made by other users. This model is then used to predict items (or ratings for items) that the user may have an interest in. Matrix factorization is a class of collaborative filtering algorithms used in recommender systems. Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices.\n",
    "\n",
    "### Content-based filtering approaches utilize a series of discrete characteristics of an item in order to recommend additional items with similar properties. \n",
    "\n",
    "### These approaches are often combined. It's called hybrid. \n",
    "* ### Weighted: linear combination or voting\n",
    "* ### Switching model on some criteria\n",
    "* ### Feature based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Entropy, Kullback-Leibler divergence and Cross-Entropy\n",
    "### \\begin{align} \n",
    "H(P) & = -E_{x\\sim P} [\\log(P(x))] \\\\\n",
    "D_{KL}(P||Q) & = E_{x\\sim P} [\\log(P(x)) - \\log(Q(x))] \\\\\n",
    "H(P, Q) & = -E_{x\\sim P} [\\log(Q(x))]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Batch Normalization \n",
    "### Very deep models involve the composition of several layers. The gradient tells how to update each parameter, under the assumption that the other layers do not change. In practice, we update all the layers simultaneously. When we make the update, unexpected results can happen, such as vanishing or exploding gradient. To increase the stability of a neural network, batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation. Consequently, batch normalization adds two trainable parameters to each layer, so the normalized output is multiplied by a “standard deviation” parameter and add a “mean” parameter. In other words, batch normalization lets SGD do the denormalization by changing only these two weights for each activation, instead of losing the stability of the network by changing all the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Gradient boosting\n",
    "### Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. \n",
    "\n",
    "### It has same advantages as Random Forest, but require less memory and faster (trees are usually smaller). Disadvantages: a lot of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Linear model\n",
    "### Linear regression is a linear approach to modelling the relationship between a scalar response (dependent variable) and one or more explanatory variables (or independent variables). \n",
    "\n",
    "### The binary logistic regression is used to estimate the probability of a binary response based on one or more predictor variables.\n",
    "\n",
    "### Very fast, can be used with big amount of features ($\\sim10^6$), the coefficients before the features can be interpreted.\n",
    "\n",
    "### Poor work in problems in which the dependence of responses on characteristics is complex, nonlinear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Multicollinearity\n",
    "### In statistics, multicollinearity is a phenomenon in which one of the features can be linearly predicted from the others with a substantial degree of accuracy. In this situation the coefficient estimates of the regression may change erratically in response to small changes in the model or the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## What are the different techniques to remove outliers?\n",
    "\n",
    "* ### Cap at threshold\n",
    "* ### Transform to reduce skew (using Box-Cox or similar)\n",
    "* ### Remove outliers if you're certain they are anomalies or measurement errors\n",
    "* ### Some models are capable to work stable with outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## What are 3 ways of reducing dimensionality?\n",
    "* ### Removing collinear features\n",
    "* ### Performing PCA, ICA, or other forms of algorithmic dimensionality reduction\n",
    "* ### Combining features with feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## How do you treat missing values?\n",
    "* ### Search for pattern in missing data (e.g. missing values exist only for some group)\n",
    "* ### Impute it with some value given some group (mean, median, min, max). Maybe create signaling variable\n",
    "* ### Some algorithms can work with missing values\n",
    "* ### If variable consist almost completely of missing values it can be dropped or its very rare and you have enough data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Logit and Probit\n",
    "### Logit and probit differ in how they define f(). The logit model uses something called the cumulative distribution function of the logistic distribution. The probit model uses something called the cumulative distribution function of the standard normal distribution to define f()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## In SVM why is there a need to maximize the margin between support vectors?\n",
    "### Hinge loss penalaized object with margin less than one, even if it is positive (object is classified correctly). \n",
    "\\begin{align}\n",
    "L(y) = \\max(0, 1 - M(y, t)) \\\\\n",
    "M(y,t) = t y = t w x\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Pros and cons of SVM\n",
    "\n",
    "1. ### Their dependence on relatively few support vectors means that they are very compact models, and take up very little memory and the prediction phase is very fast.\n",
    "2. ### Their integration with kernel methods makes them very versatile, able to adapt to many types of data.\n",
    "\n",
    "\n",
    "1. ### Slow, O($n^2$) at best.\n",
    "2. ### The results are strongly dependent on a suitable choice for the softening parameter C. This must be carefully chosen via cross-validation, which can be expensive as datasets grow in size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Coefficient of determination\n",
    "### R squared is the proportion of the variance in the dependent variable that is predictable from the independent variable. R is [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ROC AUC\n",
    "### A receiver operating characteristic curve, i.e., ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Confusion Matrix in Machine Learning\n",
    "\n",
    "* ### Positive (P) : Observation is positive (for example: is an apple). \n",
    "* ### Negative (N) : Observation is not positive (for example: is not an apple). \n",
    "* ### True Positive (TP) : Observation is positive, and is predicted to be positive. \n",
    "* ### False Negative (FN) : Observation is positive, but is predicted negative. Type 2 error.\n",
    "* ### True Negative (TN) : Observation is negative, and is predicted to be negative. \n",
    "* ### False Positive (FP) : Observation is negative, but is predicted positive. Type 1 error.\n",
    "\n",
    "\n",
    "## \\begin{align} \n",
    "Accuracy  = \\frac {TP + FN} {P + N} \\\\\n",
    "Recall\\ (Sensitivity)  = \\frac {TP} {TP + FN} \\\\\n",
    "Precision  = \\frac {TP} {TP + FP} \\\\\n",
    "F  = 2\\frac {Recall Precision} {Recall + Precision}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Pros and cons of decision trees\n",
    "\n",
    "1. #### Creation of clear rules of classification, understandable to man. This property is called the interpretability of the model. Easy to visualize.\n",
    "2. #### Rapid learning and forecasting processes.\n",
    "3. #### Work with catedorial data. Don't need much preprocessing. Outliers is not a problem.\n",
    "\n",
    "\n",
    "1. #### The problem of finding the optimal decision tree (minimal in size and capable of classifying the sample without errors) is NP-complete, so in practice heuristics are used such as greedy search for a characteristic with the maximum information growth that does not guarantee the finding of a globally optimal tree.\n",
    "2. #### Low bias and high varience tends to be overfitted.\n",
    "3. #### The model can only interpolate, but not extrapolate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Pros and cons of random forest\n",
    "\n",
    "1. ### Easy to parralaze training and prediction.\n",
    "2. ### Rapid learning and forecasting processes.\n",
    "3. ### Work with catedorial data. Don't need much preprocessing. Outliers is not a problem.\n",
    "4. ### Out-of-bag error estimation.\n",
    "5. ### Comapare to decision trees has lower varience.\n",
    "\n",
    "\n",
    "1. ### The problem of finding the optimal decision tree (minimal in size and capable of classifying the sample without errors) is NP-complete, so in practice heuristics are used such as greedy search for a characteristic with the maximum information growth that does not guarantee the finding of a globally optimal tree.\n",
    "2. ### High memory and time cost.\n",
    "3. ### The model can only interpolate, but not extrapolate.\n",
    "4. ### Bad for sparse data (> 10^5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Pros and cons of knn\n",
    "\n",
    "1. ### You can adapt to the desired task by choosing a metric or kernel (the kernel can specify a similarity operation for complex objects, and the kNN approach itself remains the same).\n",
    "2. ### Good interpretation, one can explain why the test case was classified in this way.\n",
    "\n",
    "1. ### Not with high dimensional data because of curse of dimension.\n",
    "2. ### Hard to choose hyper parametars and right distance between objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Central limit theorem and Law of large numbers\n",
    "### In probability theory, the central limit theorem establishes that when independent random variables are added, their properly normalized sum tends toward a normal distribution even if the original variables themselves are not normally distributed.\n",
    "\n",
    "### According to the law, the average of the results obtained from a large number of trials should be close to the expected value, and will tend to become closer as more trials are performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## A/B testinng\n",
    "### A/B testing is a way to compare two versions of a single variable, typically by testing a subject's response to variant A against variant B, and determining which of the two variants is more effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Statistical power and how much data should we used?\n",
    "### The power of a binary hypothesis test is the probability that the test correctly rejects the null hypothesis when a specific  alternative hypothesis is true. As the power increases, there is a decreasing probability of a type II error, also referred to as the  false negative ratio.\n",
    "\n",
    "### Power nearly always depends on the following three factors: the statistical significance criterion used in the test, the  magnitude of the effect of interest in the population and the sample size used to detect the effect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Eigenvalues and eigenvectors\n",
    "### In linear algebra, an eigenvector vector of a linear transformation, that in finite-dimensional case, can be represented as a square matrix, is a non-zero vector that changes by only a scalar factor when that linear transformation is applied to it. And this scaler value called eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## PCA\n",
    "### PCA is mathematically defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some projection of the data comes to lie on the first coordinate, the second greatest variance on the second coordinate, and so on.\n",
    "### It turns out that thess coordinates are eigenvectors of $X^{T}X$,  with the maximum values for the quantity in brackets given by their corresponding eigenvalues. Thus the weight vectors are eigenvectors of $X^{T}X$. So final transformation is $T=XW$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Project steps\n",
    "1. ### Understand business problem.\n",
    "2. ### Explore data.\n",
    "3. ### Prepare data for modeling by removing outliers, treating missing values and transforming variables.\n",
    "4. ### Start running the model, analyse the result and tweak the approach.\n",
    "5. ### Validate the  model on new dataset.\n",
    "6. ### Track the result to analyse performanse of the model over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## How to define number of clusters of k-means?\n",
    "\n",
    "### You can define distortion function which measures the sum of squared distances between each training example and the cluster centroid to which it has been assigned. You can when plot dependence of distortion function from number of clusters. And choose point where it stops decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Text features\n",
    "### We can use characters, words and n-gramms.\n",
    "* ### Term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.Term frequency is the ratio of the number of occurrences of some word to the total number of words in the document. The inverse document frequency is a measure of how much information the word provides, that is, whether the term is common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word, obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient.\n",
    "* ### Word embedding is the collective name for a set of techniques in natural language processing where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension.\n",
    "\n",
    "### For text preprocessing we usually move it to lowercase, remove punctuation, numbers, special symbols and then apply stemming or lemmatization. It depends on language. Lemmatization for Russian and stemming for English. For example words: 'go', 'goes', 'went' have 1 lemma and may have 2 stem.\n",
    "* ### In linguistic stemming is the process of reducing inflected words to their word stem, base or root form.\n",
    "* ### Lemmatization in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.\n",
    "\n",
    "### Text augmentation.\n",
    "* ### Translate back and forth.\n",
    "* ### Language model (require a lot of data).\n",
    "* ### Divide by sentences and make new examples from random sentences.\n",
    "* ### Create new examples by change part of words to their nearest embedding or synonyms.\n",
    "* ### Paraphrases generation using syntax trees transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Word2vec problem\n",
    "### Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. It takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.\n",
    "* ### Out of vocabulary\n",
    "* ### Homonyms are words which sound alike or are spelled alike, but have different meanings. \n",
    "* ### Mostly captures semantic features.\n",
    "* ### The embeddings are context-independed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ELMo (Deep contextualized representation)\n",
    "### Embeddings from language models. CharCNN 2 LSTM predictinig next or previous word\n",
    "* ### Embeddings from CharCNN any word (no out of vocabulary problem)\n",
    "* ### Actually 2 models (forward & backward)\n",
    "* ### Deep embeddings vector - weighted sum of initial CharCNN embedding and 2 output of LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### ULMfit\n",
    "#### Universal Language Model fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### BERT\n",
    "#### Bidirectional Encoder Representation from Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Abstract Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Array\n",
    "### Arrays are index based data structure where each element associated with an index. Basically, an array is a set of similar data objects stored in sequential memory locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Linked list\n",
    "### A linked list is a linear collection of data elements. A linked list relies on a distributed representation in which a lightweight object, known as a node, is allocated for each element. Each node maintains a reference to its element and one or more reference to neighboring nodes in order to collectively represent the linear order of the sequence.\n",
    "\n",
    "### Elements of linked list cannot be efficiently accessed by a numeric index, but insertions and deletions at interior position faster than for an array. The bounds on operation is not amortized. They use more memory than arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Stack\n",
    "### A stack is a collection of objects that are inserted and removed according to the last-in, first-out principle. A user may insert object into a stack at any time, but may only access or remove the most recently inserted object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Queue\n",
    "### Queue is a collection of object that are inserted and removed according to the first-in, first-out principle. Elements can be inserted at any time, but only element that has been in the queue the longest can be next removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Tree\n",
    "### A tree is an abstarct data type that stores elements hierarchically. With exception of the top element, each element in a tree has a parent element and zero or more children elements.\n",
    "\n",
    "### A tree is ordered if there is a meaningful linear order among the children of each node.  A binary tree is an ordered tree with at most two children for each node.\n",
    "\n",
    "### In a preorder traversal of a tree, the root of tree is visited first and then the subtrees rooted at its children are traversed recursively.  In postorder traversal we recursively traverses the subtrees rooted at the children of the root first, and then visit the root. Another common approach is to traverse a tree so that we visit all position at depth d before the positions at depth d + 1. Its called breadth-first traversal. For binary tree we can visit a position between the recursive traversal of its left and right subtree. Its called inorder traversal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## AVL Tree\n",
    "### Height-Balance Property: For every position p of T , the heights of the children of p differ by at most 1.\n",
    "\n",
    "### Any binary search tree T that satisfies the height-balance property is said to be an AVL tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Splay tree\n",
    "### A splay tree is a self-adjusting binary search tree with the additional property that recently accessed elements are quick to access again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Heap\n",
    "### A heap is a specialized tree based data structure that satisfies heap property: if P is a parent node of C, then key (the value) of P is either greater than or equal to the key of C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hash table\n",
    "### Hash table is a data structure that implements an associative array ADT, a structure that can map keys to values. A hash table uses a hash function to compute an index into an array of buckets, from which the desired value can be found. Search, insert and delete operation is O(1), but required extra space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Disjoint-set\n",
    "### A disjoint-set is a data structure that tracks a set of elements partitioned into a number of disjoint (non-overlapping) subsets. It provides near-constant-time operations to add new sets, to merge existing sets, and to determine whether elements are in the same set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
