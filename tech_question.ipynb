{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How yo address/handle/prevent overfitting?\n",
    "### Firstly you need to spot overfitting. You can do this by dividing your initial dataset in train and test sets. When you train your model on train set and see the performence on test set. If your model does much better on the train set than on the test set, then you are likely overfitting.\n",
    "### You can try to remove useless features. Some algorithms can assign importance to feature. Thus you can drop less important. Obviously with validation of performance.\n",
    "### When you’re training a learning algorithm iteratively, you can measure how well each iteration of the model performs. And stop training then validation error stop decreasing. This called early stopping.\n",
    "### Tune the complexity of your model. For example, regularization - process of adding restriction on the weights of your model. You can force you tree model to have some max depth or restrict minimum amount of sample in leaves. \n",
    "### Ensembling: bagging (combine strong model independently) and boosting (combine  weak learners into a single strong learner).\n",
    "### Try to gather more data or make data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff\n",
    "### Bias is the difference between your model's expected predictions and the true values.\n",
    "### Variance refers to your algorithm's sensitivity to specific sets of training data.\n",
    "### Bias-Variance Tradeoff refer to fact what Low variance (high bias) algorithms tend to be less complex, with simple or rigid underlying structure. On the other hand, low bias (high variance) algorithms tend to be more complex, with flexible underlying structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bieses in data\n",
    "### Sample bias occurs when the sample doesn’t reflect the population. You can resample you sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "### Naive Bayes is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. So a particular feature in a class is unrelated to the presence of any other feature. Thus the probability of class given feature vector is probability of class multiplied by probability of feature given class divided by evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree\n",
    "### As name goes decision tree use tree-like structure. It's defined recursively at each node there is a split on one of features, for example age is bigger than 40, true goes to left and false to right. And this process continues until some stop criteria is met, then this node become a leaf with value of the most frequent class or average target value depending on task. The split is chosen to maximize information gain, which is entropy of root node minus weighted average entropy in subtree nodes, but it can be defined differently. The splitting stops when the entropy of the node is 0, but usually other criteria can be used, for example we can restrict the tree height with some max values or IG should be bigger than some threshould."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "### K-nearest neighbors algorithm is a non-parametric method used for classification and regression. In both cases, the prediction consists of the k closest training examples in the feature space. Then in classification an object is assigned to the most common class among its k nearest neighbors. In regression task average of the values of k nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means\n",
    "### K-means is clustering algorithms that aims to partition observations into k clusters in such way that mean euclidean distance between each point in cluster and cluster centroid is minimal. Firstly initialize set of k centoids randomly. When 2 iterative steps: assign each observation to the cluster whose centroid has the least squared Euclidean distance after that calculate the new centroids of the observations in the new clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption of linear regression\n",
    "* ### First, linear regression needs the relationship between the independent and dependent variables to be linear. \n",
    "* ### Secondly, the linear regression analysis requires all variables to be normally distributed.\n",
    "* ### Thirdly, linear regression assumes that there is little or no multicollinearity in the data.  \n",
    "* ### Fourthly, linear regression analysis requires that there is little or no autocorrelation in the data.  Autocorrelation occurs when the residuals are not independent from each other. \n",
    "* ### The last assumption of the linear regression analysis is homoscedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comon errors in data\n",
    "* ### Wrong date format\n",
    "* ### Multipal representation\n",
    "* ### Duplicate records\n",
    "* ### Redundant data\n",
    "* ### Mixed numerical scales\n",
    "* ### Spelling errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle missing data\n",
    "### Before dealing with missing value, we have to understand the reason why data goes missing. Two possible reasons are that the missing value depends on the hypothetical value (e.g. People with high salaries generally do not want to reveal their incomes in surveys) or missing value is dependent on some other variable’s value (Let’s assume that females generally don’t want to reveal their ages! Here the missing value in age variable is impacted by gender variable)\n",
    "### Time-Series Specific Methods: last observation carried forward & next observation carried backward, linear interpolation, seasonal adjustment + linear interpolation\n",
    "### Impute Mean, Median and Mode\n",
    "### For categorical: impute mode, missing values can be treated as a separate category by itself.\n",
    "### KNN to find similar values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
