{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How you address/handle/prevent overfitting?\n",
    "### Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data.\n",
    "### Firstly you need to spot overfitting. You can do this by dividing your initial dataset in train and test sets. When you train your model on train set and see the performence on test set. If your model does much better on the train set than on the test set, then you are likely overfitting.\n",
    "### Tune the complexity of your model. Regularization refers to a broad range of techniques for artificially forcing your model to be simpler. For example, adding restriction on the weights of your model. You can force you tree model to have some limited max depth or restrict minimum amount of sample in leaves. \n",
    "### You can try to remove irrelevant features. The idea is simple some of your variables do not contain information about target variable. So model use them just to better fit the training set. Some algorithms can assign importance to feature. Thus you can drop less important. Obviously with validation of performance.\n",
    "### When you’re training a learning algorithm iteratively, you can measure how well your model performs on each iteration. And stop training then validation error stop decreasing. This called early stopping.\n",
    "### Ensembling: bagging (combine strong model independently) and boosting (combine  weak learners into a single strong learner). Bagging uses complex base models and tries to \"smooth out\" their predictions, while boosting uses simple base models and tries to \"boost\" their aggregate complexity.\n",
    "### Try to gather more data or make data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff\n",
    "### Bias is the difference between your model's expected predictions and the true values.\n",
    "### Variance refers to your algorithm's sensitivity to specific sets of training data.\n",
    "### Bias-Variance Tradeoff refer to fact what Low variance (high bias) algorithms tend to be less complex, with simple or rigid underlying structure. On the other hand, low bias (high variance) algorithms tend to be more complex, with flexible underlying structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bieses in data\n",
    "### Sample bias occurs when the sample doesn’t reflect the population. You can resample you sample.\n",
    "### Selection bias is the bias introduced by the selection of individuals, groups or data for analysis in such a way that proper randomization is not achieved, therefore the obtained sample is not representing the population intended to be analyzed.\n",
    "### Confirmation bias is the tendency to search and interpret information in a way that confirms preexisting beliefs or hypotheses, rather than explore a hypothesis. It looks for data to support the opinion rather than forming a theory and planning an experiment to address if a hypothesis is supported by data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confounding variable\n",
    "### Confounding variable is a variable that influences both the dependent variable and independent variable, causing a pseudo association. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "### Variance thresholds remove features whose values don't change much from observation to observation.\n",
    "### Remove features that are highly correlated with others.\n",
    "### Backward stepwise search: start with all features in your model and then remove one at a time until performance starts to drop substantially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curse of Dimensionality\n",
    "### The curse of dimensionality refers to various phenomena that arise when working with high-dimensional data. The common theme of these problems is that when the dimensionality increases, number of possible distinct conﬁgurations of a set of variables increases exponentially, so typical grid cell has no training example associated with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model\n",
    "### Linear regression is a linear approach to modelling the relationship between a scalar response (dependent variable) and one or more explanatory variables (or independent variables). \n",
    "\n",
    "### The binary logistic regression is used to estimate the probability of a binary response based on one or more predictor variables.\n",
    "### Very fast, can be used with big amount of features ($\\sim10^6$), the coefficients before the features can be interpreted.\n",
    "\n",
    "### Poor work in problems in which the dependence of responses on characteristics is complex, nonlinear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "### Motivation is pretty simple. Suppose we are given data points each belong to one of two classes and we want to separate points of different classes with hyperplane. This is called a linear classifier. There are many hyperplanes that might classify the data. One reasonable choice to choose the hyperplane so that the distance from it to the nearest data point of each class is maximized. It turns out that the hyperplane depends on the training examples that lies on the boundary of the margin between classes and on the vectors that mistakenly classified. This vectors called support vectors.\n",
    "1. ### Their dependence on relatively few support vectors means that they are very compact models, and take up very little memory and the prediction phase is very fast.\n",
    "2. ### Their integration with kernel methods makes them very versatile, able to adapt to many types of data.\n",
    "\n",
    "\n",
    "1. ### Slow, O($n^2$) at best.\n",
    "2. ### The results are strongly dependent on a suitable choice for the softening parameter C. This must be carefully chosen via cross-validation, which can be expensive as datasets grow in size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "### Naive Bayes is a classification technique based on Bayes’ Theorem with an assumption of independence among features given a class. So a particular feature in a class is unrelated to the presence of any other feature. Thus the probability of class given feature vector is probability of class (so its prior distribution of classes) multiplied by product of conditional probabilities of each feature given class (so its likelihood) divided by evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree\n",
    "### As name goes decision tree use tree-like structure. It's defined recursively at each node there is a binary split on one of features, for example age is bigger than 40, true goes to left and false to right. And this process continues until some stop criteria is met, then this node become a leaf with value of the most frequent class or average target value depending on task. The split is chosen to maximize information gain, which is entropy of root node minus weighted average entropy in subtree nodes. Intuitively after each split the subtrees become more homogeneous. The splitting stops when the entropy of the node is 0, but usually other criteria can be used, for example we can restrict the tree height with some max values or IG should be bigger than some threshould.\n",
    "1. ### Creation of clear rules of classification, understandable to man. This property is called the interpretability of the model. Easy to visualize.\n",
    "2. ### Rapid learning and forecasting processes.\n",
    "3. ### Work with catedorial data. Don't need much preprocessing. Outliers is not a problem.\n",
    "\n",
    "\n",
    "1. ### The problem of finding the optimal decision tree (minimal in size and capable of classifying the sample without errors) is NP-complete, so in practice heuristics are used such as greedy search for a characteristic with the maximum information growth that does not guarantee the finding of a globally optimal tree.\n",
    "2. ### Low bias and high varience tends to be overfitted.\n",
    "3. ### The model can only interpolate, but not extrapolate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GB Trees\n",
    "### Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion optimizing of an arbitrary differentiable loss function.\n",
    "### If we have training set with corresponding values target variable, the goal is to find an approximation function that minimizes the expected value of specified differentiable loss function. The gradient boosting method seeks an approximation in the form of a weighted sum of functions called base learners, typically decision trees. It does so by starting with a model, consisting of a constant function and incrementally expands it in a greedy fashion. The idea is to apply a steepest gradient descent on loss function with respect to an approximation on previous step. This way we have pseudo-residuals for each training example and we can fit a base learner to pseudo-residuals and chose the weight and update our model.\n",
    "\n",
    "### Same as trees\n",
    "\n",
    "1. ### Out-of-bag error estimation.\n",
    "2. ### Comapare to decision trees has lower varience.\n",
    "\n",
    "\n",
    "1. ### High memory and time cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "### K-nearest neighbors algorithm is a non-parametric method used for classification and regression. In both cases, firstly we defined some metric to measure the distance between pair of points. Then we find out k closest training points to the point, which prediction we are seeking. Then in classification task an object is assigned to the most common class among its k nearest neighbors. In regression task average of the values of k nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means\n",
    "### K-means is clustering algorithms that aims to partition observations into k clusters in such way that mean euclidean distance between each point in cluster and cluster centroid is minimal. Firstly initialize set of k centoids randomly. When 2 iterative steps: assign each observation to the cluster whose centroid has the least squared Euclidean distance after that calculate the new centroids of the observations in the new clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption of linear regression\n",
    "* ### First, linear regression needs the relationship between the independent and dependent variables to be linear. \n",
    "* ### Secondly, the linear regression analysis requires all variables to be normally distributed.\n",
    "* ### Thirdly, linear regression assumes that there is little or no multicollinearity in the data.  \n",
    "* ### Independence of errors. This assumes that the errors of the response variables are uncorrelated with each other.\n",
    "* ### Constant variance (homoscedasticity). This means that different values of the response variable have the same variance in their errors, regardless of the values of the predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comon errors in data\n",
    "* ### Wrong date format\n",
    "* ### Multipal representation\n",
    "* ### Duplicate records\n",
    "* ### Redundant data\n",
    "* ### Mixed numerical scales\n",
    "* ### Spelling errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle missing data\n",
    "### Before dealing with missing value, we have to understand the reason why data goes missing. Two possible reasons are that the missing value depends on the hypothetical value (e.g. People with high salaries generally do not want to reveal their incomes in surveys) or missing value is dependent on some other variable’s value (Let’s assume that females generally don’t want to reveal their ages! Here the missing value in age variable is impacted by gender variable)\n",
    "### Time-Series Specific Methods: last observation carried forward & next observation carried backward, linear interpolation, seasonal adjustment + linear interpolation\n",
    "### Impute Mean, Median and Mode\n",
    "### For categorical: impute mode, missing values can be treated as a separate category by itself.\n",
    "### KNN to find similar values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ROC\n",
    "### A receiver operating characteristic curve is a graphical plot that illustrates the diagnostic ability of a binary classifier as its discrimination threshold is varied. It is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The true-positive rate is ratio of true positive to all positive. It's also known as sensitivity, recall. The false-positive rate is also known as probability of false alarm and can be calculated as (1 − specificity). So its ratio of false positive to all negative examples. So then threshold is < 0, recall = 1 (our algorithm predicts only positive classes) and specificity = 0. With threshold > 1 recall = 0) and specificity = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words & N-gram\n",
    "### The bag-of-words model is a simplifying representation used in natural language processing. In this model, a text is represented as the bag of its words, disregarding grammar and even word order but keeping multiplicity. \n",
    "### N-gram is a contiguous sequence of n items from a given sample of text or speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/B testing\n",
    "### A/B testing is a way to compare two versions of a single variable, typically by testing a subject's response to variant A against variant B, and determining which of the two variants is more effective.\n",
    "### In statistical hypothesis testing a result has statistical significance when it is very unlikely to have occurred given the null hypothesis. Significance level, α, is the probability of rejecting the null hypothesis, given that it were true; and the p-value of a result, p, is the probability of obtaining a result at least as extreme, given that the null hypothesis were true.\n",
    "### Randomization is at the core of experimentation because it balances out these confounding variables. By assigning 50% of users to a control group and 50% of users to a treatment group randomly, you can ensure that the roughly same level of all possible confounding variable assign to groups.\n",
    "### Multiple Comparisons. The more metrics you are measuring, the more likely you are to get at least one false positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop\n",
    "### Hadoop is a framework that allows distributed processing of large data sets across clusters of computers using simple programming models. The Hadoop application works in an environment that provides distributed storage and computation across clusters of computers. MapReduce is a parallel programming model for writing distributed applications. The Hadoop Distributed File System provides a distributed file system that is designed to run on commodity hardware.\n",
    "### HDFS holds very large amount of data and provides easier access. To store such huge data, the files are stored across multiple machines. These files are stored in redundant fashion to rescue the system from possible data losses in case of failure.\n",
    "### The MapReduce algorithm contains two important tasks, namely Map and Reduce. Map takes a set of data and converts it into another set of data, where individual elements are broken down into tuples (key/value pairs). Secondly, reduce task, which takes the output from a map as an input and combines those data tuples into a smaller set of tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## NoSQL\n",
    "### NoSQL is a name given to a category of databases that defy traditional relational database paradigms to focus on performance and scalability. Typically relational databases take advantage of two key feature: Schemas and Relationships. So schemas allow you to control the data coming into a database and validate the data and relationships allows you to enforce connections between various tables. NoSQL databases shift the burden of Schemas and Relationships to the application-tier of your solution. ACID (Atomicity, Consistency, Isolation, and Durability) versus BASE (Basically Available, Soft State, Eventual consistency).\n",
    "### A key-value store is a special type of NoSQL Database that stores data as collections of key-value pairs. Key-value stores are popular because they are considered the simplest to implement among NoSQL stores. They are the foundation behind many NoSQL databases and the store type shares a lot of basic features with other NoSQL store types.\n",
    "### A Document-based NoSQL store is a special type of database that extends the functionality found in a typical key-value NoSQL store. In a key-value store, you are primarily restricted to making queries based on the key with the value of each entity remaining opaque. In a document store, you can perform operations and queries based on the keys or values.\n",
    "### Relational databases often store data in a row-oriented fashion by including relevant “row” data together. Column stores are similar to relational-databases but they store data in a column-oriented fashion. Queries that perform operations on the same column across many items will see a performance increase with this type of database."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
